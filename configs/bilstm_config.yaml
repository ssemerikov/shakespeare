# Bi-LSTM Model Configuration - Optimized for 70% accuracy

model:
  architecture: bilstm
  embedding_dim: 256  # Increased for better representation
  lstm_units: [512, 256]  # Increased capacity
  dense_units: 1024  # Increased hidden layer
  dropout_rates: [0.1, 0.2, 0.3]  # Reduced dropout to allow more learning
  bidirectional: true
  use_pretrained_embeddings: false
  trainable_embeddings: true

data:
  text_file: Shakespeare.txt
  vocab_size: 5000  # Reduced vocab for higher accuracy (focus on common words)
  sequence_length: 40  # Increased context window
  batch_size: 128  # Larger batches for stability
  train_split: 0.8

training:
  epochs: 50  # More epochs for convergence
  initial_lr: 0.002  # Slightly higher LR for faster initial learning
  optimizer: adam
  loss: sparse_categorical_crossentropy
  clipnorm: 1.0
  label_smoothing: 0.0  # Removed to allow sharper predictions

callbacks:
  early_stopping:
    patience: 5
    monitor: val_loss
  lr_scheduler:
    factor: 0.5
    patience: 2
    min_lr: 1.0e-6
  model_checkpoint:
    save_best_only: true
    monitor: val_accuracy

inference:
  temperature: 0.8
  top_k: 40
  num_words: 100

paths:
  model_save_path: models/best_bilstm_model.h5
  log_dir: logs
  embeddings_dir: data
