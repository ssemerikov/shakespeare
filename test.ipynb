{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Character-Level Bi-LSTM Shakespeare Text Generation: Test Suite\n",
    "\n",
    "**Purpose**: Comprehensive testing and evaluation notebook supporting both:\n",
    "1. Training models from scratch\n",
    "2. Testing pre-trained models\n",
    "\n",
    "**Model Architecture**:\n",
    "- Embedding: 128 dim (70 char vocab)\n",
    "- Bi-LSTM 1: 256 units with return_sequences=True\n",
    "- Bi-LSTM 2: 128 units with return_sequences=False\n",
    "- Dense: 256 units + ReLU activation\n",
    "- Output: 70 units (character vocab) + Softmax\n",
    "\n",
    "**Key Metrics**:\n",
    "- Test Accuracy: 49.90% (target: 70%)\n",
    "- Perplexity: 5.69 (excellent, <10)\n",
    "- Top-5 Accuracy: 80.40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries and set configuration\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, Bidirectional, Embedding\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Print library versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'sequence_length': 100,\n",
    "    'batch_size': 512,\n",
    "    'embedding_dim': 128,\n",
    "    'lstm_units': 256,\n",
    "    'lstm_units_2': 128,\n",
    "    'dense_units': 256,\n",
    "    'dropout_rate': 0.2,\n",
    "    'epochs': 30,\n",
    "    'validation_split': 0.15,\n",
    "    'learning_rate': 0.002,\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Configuration loaded: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper functions for character-level data preprocessing\"\"\"\n",
    "\n",
    "def load_shakespeare_text(file_path='Shakespeare.txt'):\n",
    "    \"\"\"Load raw Shakespeare text\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    \"\"\"Create character-to-index and index-to-character mappings\"\"\"\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "    idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "def create_training_sequences(text, char_to_idx, seq_length=100, step=3):\n",
    "    \"\"\"Create (input sequence, target character) pairs for training\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        char_to_idx: Character to index mapping\n",
    "        seq_length: Length of input sequences\n",
    "        step: Step size for sliding window (3 = faster training)\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences [N, seq_length]\n",
    "        y: Target characters [N]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(0, len(text) - seq_length, step):\n",
    "        sequence = text[i:i + seq_length]\n",
    "        target = text[i + seq_length]\n",
    "        \n",
    "        try:\n",
    "            X.append([char_to_idx[c] for c in sequence])\n",
    "            y.append(char_to_idx[target])\n",
    "        except KeyError:\n",
    "            # Skip if character not in mapping\n",
    "            continue\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_or_create_char_mappings(model_mappings_path='models/char_mappings.pkl',\n",
    "                                  text_file='Shakespeare.txt'):\n",
    "    \"\"\"Load pre-trained character mappings or create new ones\"\"\"\n",
    "    \n",
    "    if os.path.exists(model_mappings_path):\n",
    "        with open(model_mappings_path, 'rb') as f:\n",
    "            mappings = pickle.load(f)\n",
    "            return mappings['char_to_idx'], mappings['idx_to_char']\n",
    "    else:\n",
    "        text = load_shakespeare_text(text_file)\n",
    "        char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "        return char_to_idx, idx_to_char\n",
    "\n",
    "def save_char_mappings(char_to_idx, idx_to_char, save_path='models/char_mappings.pkl'):\n",
    "    \"\"\"Save character mappings for later use\"\"\"\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'char_to_idx': char_to_idx,\n",
    "            'idx_to_char': idx_to_char\n",
    "        }, f)\n",
    "    print(f\"‚úì Mappings saved to {save_path}\")\n",
    "\n",
    "print(\"‚úì Character-level preprocessing functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-04",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Preparation\n",
    "\n",
    "This section handles loading Shakespeare text, creating character mappings, \n",
    "and preparing training sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Shakespeare text and create character mappings\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load text\n",
    "text = load_shakespeare_text('Shakespeare.txt')\n",
    "print(f\"\\n‚úì Loaded {len(text):,} characters from Shakespeare.txt\")\n",
    "print(f\"  Sample: {text[:100]}\")\n",
    "\n",
    "# Create character mappings\n",
    "char_to_idx, idx_to_char = create_char_mappings(text)\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print(f\"\\n‚úì Character mappings created\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Sample characters: {sorted(list(set(text)))[:30]}\")\n",
    "print(f\"  Unique characters: {list(char_to_idx.items())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create training sequences with specified configuration\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TRAINING SEQUENCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sequences with step=3 for faster creation\n",
    "X, y = create_training_sequences(\n",
    "    text=text,\n",
    "    char_to_idx=char_to_idx,\n",
    "    seq_length=CONFIG['sequence_length'],\n",
    "    step=3\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training sequences created\")\n",
    "print(f\"  Total sequences: {len(X):,}\")\n",
    "print(f\"  Input shape: {X.shape}\")\n",
    "print(f\"  Output shape: {y.shape}\")\n",
    "\n",
    "# Train/validation split\n",
    "split_idx = int(len(X) * (1 - CONFIG['validation_split']))\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\n‚úì Data split completed\")\n",
    "print(f\"  Training set: {len(X_train):,} sequences ({100*(1-CONFIG['validation_split']):.1f}%)\")\n",
    "print(f\"  Validation set: {len(X_val):,} sequences ({100*CONFIG['validation_split']:.1f}%)\")\n",
    "\n",
    "# Store for later use\n",
    "DATA_SHAPES = {\n",
    "    'X_train': X_train.shape,\n",
    "    'X_val': X_val.shape,\n",
    "    'vocab_size': vocab_size,\n",
    "    'sequence_length': CONFIG['sequence_length']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-07",
   "metadata": {},
   "source": [
    "## Section 2: Model Training\n",
    "\n",
    "Two pathways:\n",
    "1. **Train from scratch**: Full character-level Bi-LSTM training\n",
    "2. **Load pre-trained**: Use existing models/char_bilstm_best.h5\n",
    "\n",
    "Choose based on your requirements and available compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper function to build character-level Bi-LSTM model\"\"\"\n",
    "\n",
    "def build_char_bilstm_model(vocab_size, sequence_length=100):\n",
    "    \"\"\"Build character-level Bi-LSTM model matching train_char_level.py\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of character vocabulary\n",
    "        sequence_length: Length of input sequences\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(\n",
    "            vocab_size, \n",
    "            CONFIG['embedding_dim'],\n",
    "            input_length=sequence_length,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        \n",
    "        # First Bidirectional LSTM\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                CONFIG['lstm_units'],\n",
    "                return_sequences=True,\n",
    "                dropout=CONFIG['dropout_rate'],\n",
    "                recurrent_dropout=CONFIG['dropout_rate'],\n",
    "                name='lstm_1'\n",
    "            ),\n",
    "            name='bidirectional_1'\n",
    "        ),\n",
    "        \n",
    "        # Second Bidirectional LSTM\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                CONFIG['lstm_units_2'],\n",
    "                return_sequences=False,\n",
    "                dropout=CONFIG['dropout_rate'],\n",
    "                recurrent_dropout=CONFIG['dropout_rate'],\n",
    "                name='lstm_2'\n",
    "            ),\n",
    "            name='bidirectional_2'\n",
    "        ),\n",
    "        \n",
    "        # Dropout after LSTM\n",
    "        Dropout(0.3, name='dropout_lstm'),\n",
    "        \n",
    "        # Dense hidden layer\n",
    "        Dense(\n",
    "            CONFIG['dense_units'],\n",
    "            activation='relu',\n",
    "            name='dense_hidden'\n",
    "        ),\n",
    "        \n",
    "        # Final dropout\n",
    "        Dropout(0.3, name='dropout_final'),\n",
    "        \n",
    "        # Output layer (character probability distribution)\n",
    "        Dense(\n",
    "            vocab_size,\n",
    "            activation='softmax',\n",
    "            name='output'\n",
    "        )\n",
    "    ], name='CharLevel_BiLSTM')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile helper\n",
    "def compile_char_model(model, learning_rate=None):\n",
    "    \"\"\"Compile character-level model\"\"\"\n",
    "    if learning_rate is None:\n",
    "        learning_rate = CONFIG['learning_rate']\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            SparseTopKCategoricalAccuracy(k=5, name='top5_acc')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Model building functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTION 1: Train character-level Bi-LSTM from scratch\"\"\"\n",
    "\n",
    "TRAIN_FROM_SCRATCH = False  # <-- SET TO TRUE TO TRAIN\n",
    "\n",
    "if TRAIN_FROM_SCRATCH:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING FROM SCRATCH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Build fresh model\n",
    "    print(\"\\nüèóÔ∏è Building fresh Bi-LSTM model...\")\n",
    "    model = build_char_bilstm_model(vocab_size, CONFIG['sequence_length'])\n",
    "    model = compile_char_model(model)\n",
    "    \n",
    "    print(\"\\nüìã Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Setup callbacks\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('logs/char_level', exist_ok=True)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'models/char_bilstm_best_fresh.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nüöÄ Starting training...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        epochs=CONFIG['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save character mappings\n",
    "    save_char_mappings(char_to_idx, idx_to_char, \n",
    "                      'models/char_mappings_fresh.pkl')\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed!\")\n",
    "    TRAINED_MODEL = model\n",
    "    TRAINED_FROM_SCRATCH = True\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  TRAIN_FROM_SCRATCH = False - skipping training\")\n",
    "    print(\"   Set TRAIN_FROM_SCRATCH = True in cell 9 to enable training\")\n",
    "    TRAINED_FROM_SCRATCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTION 2: Load pre-trained character-level model\"\"\"\n",
    "\n",
    "LOAD_PRETRAINED = True  # <-- SET TO FALSE TO SKIP\n",
    "\n",
    "if LOAD_PRETRAINED:\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING PRE-TRAINED MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model_path = 'models/char_bilstm_best.h5'\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"\\nüì¶ Loading model from {model_path}...\")\n",
    "        model = load_model(model_path)\n",
    "        print(\"‚úì Model loaded successfully\")\n",
    "        \n",
    "        # Verify model structure\n",
    "        print(\"\\nüìã Loaded Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        TRAINED_MODEL = model\n",
    "        PRETRAINED_LOADED = True\n",
    "    else:\n",
    "        print(f\"‚úó Model not found at {model_path}\")\n",
    "        print(\"  Please train from scratch or verify model path\")\n",
    "        PRETRAINED_LOADED = False\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  LOAD_PRETRAINED = False - skipping pretrained model loading\")\n",
    "    PRETRAINED_LOADED = False\n",
    "\n",
    "# Ensure we have a model\n",
    "assert (TRAINED_FROM_SCRATCH or PRETRAINED_LOADED), \\\n",
    "    \"No model available! Set TRAIN_FROM_SCRATCH or LOAD_PRETRAINED to True\"\n",
    "\n",
    "print(\"\\n‚úì Model ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Section 3: Quantitative Evaluation\n",
    "\n",
    "Metrics evaluated:\n",
    "- **Accuracy**: Top-1 character prediction accuracy\n",
    "- **Top-5 Accuracy**: Correct character in top-5 predictions\n",
    "- **Loss**: Sparse categorical crossentropy\n",
    "- **Perplexity**: Exp(loss), measure of model confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare test set for quantitative evaluation\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use last 15% of text as test set (unseen during training)\n",
    "test_start = int(len(text) * 0.85)\n",
    "test_text = text[test_start:]\n",
    "\n",
    "print(f\"\\n‚úì Test set prepared\")\n",
    "print(f\"  Test set size: {len(test_text):,} characters\")\n",
    "print(f\"  Sample: {test_text[:100]}\")\n",
    "\n",
    "# Create test sequences\n",
    "X_test, y_test = create_training_sequences(\n",
    "    text=test_text,\n",
    "    char_to_idx=char_to_idx,\n",
    "    seq_length=CONFIG['sequence_length'],\n",
    "    step=10  # Larger step for test efficiency\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Test sequences created\")\n",
    "print(f\"  Total test sequences: {len(X_test):,}\")\n",
    "print(f\"  Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Quantitative evaluation on test set\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTITATIVE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìä Evaluating model on test set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = TRAINED_MODEL.evaluate(\n",
    "    X_test, y_test,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "# Parse results\n",
    "test_loss = results[0]\n",
    "test_acc = results[1]\n",
    "test_top5 = results[2]\n",
    "\n",
    "print(f\"\\n‚úì Evaluation completed in {eval_time:.2f}s\")\n",
    "\n",
    "# Display metrics\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"{'METRIC':<30} {'VALUE':>15} {'TARGET':>15}\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "print(f\"{'Accuracy':<30} {test_acc*100:>14.2f}% {'>70%':>15}\")\n",
    "print(f\"{'Top-5 Accuracy':<30} {test_top5*100:>14.2f}% {'>80%':>15}\")\n",
    "print(f\"{'Loss':<30} {test_loss:>15.4f} {'<2.0':>15}\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(test_loss)\n",
    "print(f\"{'Perplexity':<30} {perplexity:>15.2f} {'<10':>15}\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "# Store results for later\n",
    "EVAL_RESULTS = {\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_top5_accuracy': test_top5,\n",
    "    'test_loss': test_loss,\n",
    "    'perplexity': perplexity,\n",
    "    'eval_time': eval_time\n",
    "}\n",
    "\n",
    "# Assessment\n",
    "print(f\"\\nüìà Assessment:\")\n",
    "if test_acc >= 0.70:\n",
    "    print(f\"  üåü EXCELLENT - Exceeds 70% accuracy target!\")\n",
    "elif test_acc >= 0.60:\n",
    "    print(f\"  ‚úÖ GOOD - Strong performance, {(0.70-test_acc)*100:.1f}% below target\")\n",
    "elif test_acc >= 0.50:\n",
    "    print(f\"  ‚ö†Ô∏è  MODERATE - {(0.70-test_acc)*100:.1f}% below target, more training needed\")\n",
    "else:\n",
    "    print(f\"  ‚ùå NEEDS IMPROVEMENT - {(0.70-test_acc)*100:.1f}% below target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Section 4: Inference Speed Benchmarking\n",
    "\n",
    "Evaluate model inference performance across different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Benchmark inference speed across different batch sizes\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE SPEED BENCHMARKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [1, 32, 64, 128, 256]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if len(X_test) < batch_size:\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipping batch size {batch_size} (insufficient test samples)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n‚ö° Testing batch size {batch_size}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = TRAINED_MODEL.predict(X_test[:batch_size], verbose=0)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(20):  # Fewer runs for faster testing\n",
    "        start = time.time()\n",
    "        _ = TRAINED_MODEL.predict(X_test[:batch_size], verbose=0)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    throughput = batch_size / mean_time\n",
    "    per_sample = (mean_time * 1000) / batch_size\n",
    "    \n",
    "    benchmark_results[batch_size] = {\n",
    "        'mean_ms': mean_time * 1000,\n",
    "        'std_ms': std_time * 1000,\n",
    "        'throughput': throughput,\n",
    "        'per_sample_ms': per_sample\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚îú‚îÄ Mean time: {mean_time*1000:.2f} ¬± {std_time*1000:.2f} ms\")\n",
    "    print(f\"  ‚îú‚îÄ Per sample: {per_sample:.3f} ms\")\n",
    "    print(f\"  ‚îî‚îÄ Throughput: {throughput:.1f} samples/sec\")\n",
    "\n",
    "# Find optimal batch size\n",
    "optimal_batch = max(benchmark_results.items(), \n",
    "                   key=lambda x: x[1]['throughput'])[0]\n",
    "\n",
    "print(f\"\\nüéØ Optimal batch size: {optimal_batch}\")\n",
    "print(f\"   Throughput: {benchmark_results[optimal_batch]['throughput']:.1f} samples/sec\")\n",
    "\n",
    "BENCHMARK_RESULTS = benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize inference speed benchmarking results\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Throughput\n",
    "batch_sizes = sorted(list(BENCHMARK_RESULTS.keys()))\n",
    "throughputs = [BENCHMARK_RESULTS[bs]['throughput'] for bs in batch_sizes]\n",
    "latencies = [BENCHMARK_RESULTS[bs]['mean_ms'] for bs in batch_sizes]\n",
    "\n",
    "axes[0].bar(range(len(batch_sizes)), throughputs, color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0].set_ylabel('Throughput (samples/sec)', fontsize=12)\n",
    "axes[0].set_title('Inference Throughput by Batch Size', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(range(len(batch_sizes)))\n",
    "axes[0].set_xticklabels(batch_sizes)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, (bs, tp) in enumerate(zip(batch_sizes, throughputs)):\n",
    "    axes[0].text(i, tp + 2, f'{tp:.0f}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 2: Latency\n",
    "axes[1].plot(batch_sizes, latencies, marker='o', markersize=8, \n",
    "            linewidth=2, color='coral', label='Latency')\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Latency (ms)', fontsize=12)\n",
    "axes[1].set_title('Inference Latency by Batch Size', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Benchmark visualization saved to benchmark_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Section 5: Text Generation Quality\n",
    "\n",
    "Evaluate quality of generated text using:\n",
    "1. **Temperature sampling**: Controls randomness (0.5 = conservative, 1.0 = standard)\n",
    "2. **Multiple prompts**: Test with different Shakespeare phrases\n",
    "3. **Qualitative analysis**: Word diversity, grammar, coherence\n",
    "4. **Comparison with original**: Statistical comparison with real Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper function for text generation with temperature sampling\"\"\"\n",
    "\n",
    "def generate_text(model, seed_text, length=200, temperature=1.0, \n",
    "                 char_to_idx=None, idx_to_char=None, seq_length=100):\n",
    "    \"\"\"Generate text using character-level model with temperature sampling\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        seed_text: Starting text prompt\n",
    "        length: Number of characters to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        char_to_idx: Character to index mapping\n",
    "        idx_to_char: Index to character mapping\n",
    "        seq_length: Sequence length used in model\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    generated = seed_text\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Prepare input: take last seq_length characters\n",
    "        current_seq = generated[-seq_length:]\n",
    "        \n",
    "        # Convert to indices\n",
    "        try:\n",
    "            x = np.array([[char_to_idx.get(c, 0) for c in current_seq]])\n",
    "        except:\n",
    "            x = np.array([[char_to_idx.get(c, 0) for c in current_seq[-seq_length:]]])\n",
    "        \n",
    "        # Predict next character probabilities\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature\n",
    "        predictions = np.log(predictions + 1e-10) / temperature\n",
    "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        # Sample next character\n",
    "        next_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        generated += next_char\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"‚úì Text generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate text samples with different temperatures\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEXT GENERATION QUALITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test prompts from Shakespeare\n",
    "test_prompts = [\n",
    "    \"To be or not to be\",\n",
    "    \"O Romeo, Romeo\",\n",
    "    \"Friends, Romans, countrymen\",\n",
    "    \"All the world's a stage\",\n",
    "    \"Now is the winter\"\n",
    "]\n",
    "\n",
    "# Temperatures to test\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "\n",
    "generated_samples = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"Temperature: {temp} (conservative={temp<1.0}, standard={temp==1.0})\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    generated_samples[temp] = {}\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        # Generate text\n",
    "        generated = generate_text(\n",
    "            model=TRAINED_MODEL,\n",
    "            seed_text=prompt,\n",
    "            length=150,\n",
    "            temperature=temp,\n",
    "            char_to_idx=char_to_idx,\n",
    "            idx_to_char=idx_to_char,\n",
    "            seq_length=CONFIG['sequence_length']\n",
    "        )\n",
    "        \n",
    "        generated_samples[temp][prompt] = generated\n",
    "        \n",
    "        print(f\"\\nüìç Prompt {i+1}: '{prompt}'\")\n",
    "        print(f\"Generated:\")\n",
    "        print(generated[:200])  # Show first 200 chars\n",
    "        print()\n",
    "\n",
    "print(\"‚úì Text generation samples created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper functions for analyzing generated text quality\"\"\"\n",
    "\n",
    "def analyze_text(text):\n",
    "    \"\"\"Analyze text quality metrics\n",
    "    \n",
    "    Returns dictionary with:\n",
    "    - word_count: Total words\n",
    "    - unique_words: Unique words\n",
    "    - vocabulary_diversity: unique_words / word_count\n",
    "    - char_distribution: Counter of characters\n",
    "    - sentences: Count of sentence endings\n",
    "    - avg_word_length: Average characters per word\n",
    "    - lines: Count of line breaks\n",
    "    \"\"\"\n",
    "    # Word extraction\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_count = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    \n",
    "    # Character distribution\n",
    "    char_dist = Counter(text)\n",
    "    \n",
    "    # Sentence-like structures\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    \n",
    "    # Line breaks\n",
    "    lines = text.count('\\n')\n",
    "    \n",
    "    # Average word length\n",
    "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'unique_words': unique_words,\n",
    "        'vocabulary_diversity': unique_words / word_count if word_count > 0 else 0,\n",
    "        'char_distribution': char_dist,\n",
    "        'sentences': sentences,\n",
    "        'avg_word_length': avg_word_len,\n",
    "        'lines': lines\n",
    "    }\n",
    "\n",
    "print(\"‚úì Text analysis function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze quality of generated text\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUALITATIVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate longer sample for analysis\n",
    "print(\"\\nüî¨ Generating extended sample for analysis...\")\n",
    "long_sample = generate_text(\n",
    "    model=TRAINED_MODEL,\n",
    "    seed_text=\"To be or not to be\",\n",
    "    length=500,\n",
    "    temperature=0.8,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    seq_length=CONFIG['sequence_length']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Generated Text (500 characters, temp=0.8):\")\n",
    "print(\"‚îÄ\"*70)\n",
    "print(long_sample)\n",
    "print(\"‚îÄ\"*70)\n",
    "\n",
    "# Analyze generated text\n",
    "gen_analysis = analyze_text(long_sample)\n",
    "\n",
    "print(f\"\\nüìä Generated Text Metrics:\")\n",
    "print(f\"  ‚îú‚îÄ Total Words: {gen_analysis['word_count']}\")\n",
    "print(f\"  ‚îú‚îÄ Unique Words: {gen_analysis['unique_words']}\")\n",
    "print(f\"  ‚îú‚îÄ Vocabulary Diversity: {gen_analysis['vocabulary_diversity']:.1%}\")\n",
    "print(f\"  ‚îú‚îÄ Average Word Length: {gen_analysis['avg_word_length']:.2f} characters\")\n",
    "print(f\"  ‚îú‚îÄ Sentence-like Structures: {gen_analysis['sentences']}\")\n",
    "print(f\"  ‚îî‚îÄ Line Breaks: {gen_analysis['lines']}\")\n",
    "\n",
    "# Compare with original Shakespeare\n",
    "print(f\"\\nüìö Original Shakespeare Metrics (for comparison):\")\n",
    "orig_sample = test_text[:len(long_sample)]\n",
    "orig_analysis = analyze_text(orig_sample)\n",
    "\n",
    "print(f\"  ‚îú‚îÄ Total Words: {orig_analysis['word_count']}\")\n",
    "print(f\"  ‚îú‚îÄ Unique Words: {orig_analysis['unique_words']}\")\n",
    "print(f\"  ‚îú‚îÄ Vocabulary Diversity: {orig_analysis['vocabulary_diversity']:.1%}\")\n",
    "print(f\"  ‚îú‚îÄ Average Word Length: {orig_analysis['avg_word_length']:.2f} characters\")\n",
    "print(f\"  ‚îú‚îÄ Sentence-like Structures: {orig_analysis['sentences']}\")\n",
    "print(f\"  ‚îî‚îÄ Line Breaks: {orig_analysis['lines']}\")\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\nüìà Comparison:\")\n",
    "diversity_diff = (gen_analysis['vocabulary_diversity'] - \n",
    "                 orig_analysis['vocabulary_diversity']) * 100\n",
    "word_len_diff = gen_analysis['avg_word_length'] - orig_analysis['avg_word_length']\n",
    "\n",
    "print(f\"  ‚îú‚îÄ Diversity difference: {diversity_diff:+.1f}% \" + \n",
    "      f\"({'better' if diversity_diff > 0 else 'worse'})\")\n",
    "print(f\"  ‚îî‚îÄ Word length difference: {word_len_diff:+.2f} chars \" +\n",
    "      f\"({'more complex' if word_len_diff > 0 else 'simpler'})\")\n",
    "\n",
    "TEXT_ANALYSIS = {\n",
    "    'generated': gen_analysis,\n",
    "    'original': orig_analysis\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize text quality comparison\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Vocabulary metrics\n",
    "categories = ['Word Count', 'Unique Words', 'Vocabulary\\nDiversity (%)']\n",
    "gen_vals = [\n",
    "    gen_analysis['word_count'],\n",
    "    gen_analysis['unique_words'],\n",
    "    gen_analysis['vocabulary_diversity'] * 100\n",
    "]\n",
    "orig_vals = [\n",
    "    orig_analysis['word_count'],\n",
    "    orig_analysis['unique_words'],\n",
    "    orig_analysis['vocabulary_diversity'] * 100\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, gen_vals, width, label='Generated', color='steelblue', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, orig_vals, width, label='Original Shakespeare', color='coral', alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Value', fontsize=11)\n",
    "axes[0, 0].set_title('Vocabulary Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(categories)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Average word length\n",
    "axes[0, 1].bar(['Generated', 'Original'], \n",
    "              [gen_analysis['avg_word_length'], orig_analysis['avg_word_length']],\n",
    "              color=['steelblue', 'coral'], alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Characters', fontsize=11)\n",
    "axes[0, 1].set_title('Average Word Length', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Character distribution (top 10)\n",
    "top_chars_gen = Counter(long_sample).most_common(10)\n",
    "top_chars_orig = Counter(orig_sample).most_common(10)\n",
    "\n",
    "chars_gen, counts_gen = zip(*top_chars_gen)\n",
    "chars_orig, counts_orig = zip(*top_chars_orig)\n",
    "\n",
    "x_pos = np.arange(len(chars_gen))\n",
    "axes[1, 0].bar(x_pos, counts_gen, color='steelblue', alpha=0.8, label='Generated')\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Top 10 Character Distribution (Generated)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(chars_gen, fontsize=10)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Sentence-like structures\n",
    "axes[1, 1].bar(['Generated', 'Original'],\n",
    "              [gen_analysis['sentences'], orig_analysis['sentences']],\n",
    "              color=['steelblue', 'coral'], alpha=0.8)\n",
    "axes[1, 1].set_ylabel('Count', fontsize=11)\n",
    "axes[1, 1].set_title('Sentence-like Structures', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('text_quality_analysis.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Text quality visualization saved to text_quality_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Section 6: Model Architecture and Parameters\n",
    "\n",
    "Detailed analysis of model structure, parameter count, and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Display model architecture and parameter analysis\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüèóÔ∏è Model Summary:\")\n",
    "print(\"-\"*70)\n",
    "TRAINED_MODEL.summary()\n",
    "\n",
    "# Parameter analysis\n",
    "total_params = TRAINED_MODEL.count_params()\n",
    "trainable_params = sum([np.prod(w.shape) for w in TRAINED_MODEL.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nüìä Parameter Statistics:\")\n",
    "print(f\"  ‚îú‚îÄ Total Parameters: {total_params:,}\")\n",
    "print(f\"  ‚îú‚îÄ Trainable Parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "print(f\"  ‚îî‚îÄ Non-trainable Parameters: {non_trainable_params:,}\")\n",
    "\n",
    "# Model size estimation (assuming float32)\n",
    "model_size_mb_fp32 = (total_params * 4) / (1024 * 1024)\n",
    "model_size_mb_fp16 = (total_params * 2) / (1024 * 1024)\n",
    "model_size_mb_int8 = (total_params * 1) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüíæ Model Size Estimation:\")\n",
    "print(f\"  ‚îú‚îÄ Float32 (FP32): {model_size_mb_fp32:.2f} MB\")\n",
    "print(f\"  ‚îú‚îÄ Float16 (FP16): {model_size_mb_fp16:.2f} MB (2x compression)\")\n",
    "print(f\"  ‚îî‚îÄ Int8 (INT8):    {model_size_mb_int8:.2f} MB (4x compression)\")\n",
    "\n",
    "# Configuration summary\n",
    "print(f\"\\n‚öôÔ∏è  Configuration Summary:\")\n",
    "print(f\"  ‚îú‚îÄ Vocabulary Size: {vocab_size}\")\n",
    "print(f\"  ‚îú‚îÄ Sequence Length: {CONFIG['sequence_length']}\")\n",
    "print(f\"  ‚îú‚îÄ Embedding Dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  ‚îú‚îÄ Bi-LSTM Units: [{CONFIG['lstm_units']}, {CONFIG['lstm_units_2']}]\")\n",
    "print(f\"  ‚îú‚îÄ Dense Units: {CONFIG['dense_units']}\")\n",
    "print(f\"  ‚îî‚îÄ Dropout Rate: {CONFIG['dropout_rate']}\")\n",
    "\n",
    "# Layer-by-layer breakdown\n",
    "print(f\"\\nüìã Layer-by-Layer Breakdown:\")\n",
    "for i, layer in enumerate(TRAINED_MODEL.layers):\n",
    "    print(f\"  {i+1}. {layer.name:.<30} {layer.output_shape}\")\n",
    "\n",
    "ARCHITECTURE_INFO = {\n",
    "    'total_params': total_params,\n",
    "    'trainable_params': trainable_params,\n",
    "    'model_size_mb_fp32': model_size_mb_fp32,\n",
    "    'model_size_mb_int8': model_size_mb_int8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create visualization of model architecture\"\"\"\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Plot model architecture\n",
    "try:\n",
    "    plot_model(\n",
    "        TRAINED_MODEL,\n",
    "        to_file='model_architecture.png',\n",
    "        show_shapes=True,\n",
    "        show_layer_activations=True,\n",
    "        rankdir='TB',\n",
    "        expand_nested=True,\n",
    "        dpi=100\n",
    "    )\n",
    "    print(\"‚úì Model architecture diagram saved to model_architecture.png\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not generate architecture diagram: {e}\")\n",
    "\n",
    "# Create parameter visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parameter distribution by layer\n",
    "param_by_layer = {}\n",
    "for layer in TRAINED_MODEL.layers:\n",
    "    params = layer.count_params()\n",
    "    if params > 0:\n",
    "        param_by_layer[layer.name] = params\n",
    "\n",
    "sorted_layers = sorted(param_by_layer.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "layer_names = [name for name, _ in sorted_layers]\n",
    "layer_params = [params for _, params in sorted_layers]\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(layer_names)))\n",
    "ax1.barh(layer_names, layer_params, color=colors, alpha=0.8)\n",
    "ax1.set_xlabel('Parameters', fontsize=11)\n",
    "ax1.set_title('Parameters by Layer (Top 8)', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add values\n",
    "for i, (name, params) in enumerate(zip(layer_names, layer_params)):\n",
    "    ax1.text(params, i, f' {params:,}', va='center', fontsize=9)\n",
    "\n",
    "# Parameter composition pie chart\n",
    "param_categories = {\n",
    "    'Embedding': vocab_size * CONFIG['embedding_dim'],\n",
    "    'LSTM Layers': total_params - (vocab_size * CONFIG['embedding_dim']) - (CONFIG['dense_units'] * (CONFIG['lstm_units_2'] * 2 + 1)) - (vocab_size * CONFIG['dense_units']),\n",
    "    'Dense Hidden': CONFIG['dense_units'] * (CONFIG['lstm_units_2'] * 2 + 1),\n",
    "    'Output': vocab_size * CONFIG['dense_units']\n",
    "}\n",
    "\n",
    "ax2.pie(param_categories.values(), labels=param_categories.keys(), autopct='%1.1f%%',\n",
    "       colors=colors, startangle=90)\n",
    "ax2.set_title('Parameter Composition', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_parameters.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Model parameter visualization saved to model_parameters.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Section 7: Final Summary and Recommendations\n",
    "\n",
    "Comprehensive evaluation summary with recommendations for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate comprehensive final summary report\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL EVALUATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "report = f\"\"\"\n",
    "{'‚ñà'*70}\n",
    "CHARACTER-LEVEL BI-LSTM SHAKESPEARE TEXT GENERATION\n",
    "Comprehensive Test and Evaluation Report\n",
    "{'‚ñà'*70}\n",
    "\n",
    "1. QUANTITATIVE METRICS\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "Test Accuracy:           {EVAL_RESULTS['test_accuracy']*100:>6.2f}% (target: >70%)\n",
    "Top-5 Accuracy:          {EVAL_RESULTS['test_top5_accuracy']*100:>6.2f}%\n",
    "Loss (Sparse CE):        {EVAL_RESULTS['test_loss']:>6.4f}\n",
    "Perplexity:              {EVAL_RESULTS['perplexity']:>6.2f} (target: <10)\n",
    "\n",
    "2. MODEL PARAMETERS\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "Total Parameters:        {ARCHITECTURE_INFO['total_params']:>6,}\n",
    "Trainable Parameters:    {ARCHITECTURE_INFO['trainable_params']:>6,}\n",
    "Model Size (FP32):       {ARCHITECTURE_INFO['model_size_mb_fp32']:>6.2f} MB\n",
    "Model Size (INT8):       {ARCHITECTURE_INFO['model_size_mb_int8']:>6.2f} MB\n",
    "\n",
    "3. INFERENCE PERFORMANCE\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "Optimal Batch Size:      {optimal_batch:>6}\n",
    "Throughput:              {BENCHMARK_RESULTS[optimal_batch]['throughput']:>6.1f} samples/sec\n",
    "Per-Sample Latency:      {BENCHMARK_RESULTS[optimal_batch]['per_sample_ms']:>6.3f} ms\n",
    "\n",
    "4. TEXT QUALITY METRICS\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "Generated Text:\n",
    "  Vocabulary Diversity:  {TEXT_ANALYSIS['generated']['vocabulary_diversity']*100:>6.1f}%\n",
    "  Avg Word Length:       {TEXT_ANALYSIS['generated']['avg_word_length']:>6.2f} chars\n",
    "  Sentence Structures:   {TEXT_ANALYSIS['generated']['sentences']:>6.0f}\n",
    "\n",
    "Original Shakespeare:\n",
    "  Vocabulary Diversity:  {TEXT_ANALYSIS['original']['vocabulary_diversity']*100:>6.1f}%\n",
    "  Avg Word Length:       {TEXT_ANALYSIS['original']['avg_word_length']:>6.2f} chars\n",
    "  Sentence Structures:   {TEXT_ANALYSIS['original']['sentences']:>6.0f}\n",
    "\n",
    "5. PERFORMANCE ASSESSMENT\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Performance level\n",
    "if EVAL_RESULTS['test_accuracy'] >= 0.70:\n",
    "    rating = \"üåü EXCELLENT\"\n",
    "    desc = \"Exceeds 70% accuracy target!\"\n",
    "elif EVAL_RESULTS['test_accuracy'] >= 0.60:\n",
    "    rating = \"‚úÖ GOOD\"\n",
    "    desc = \"Strong performance, approaching target\"\n",
    "elif EVAL_RESULTS['test_accuracy'] >= 0.50:\n",
    "    rating = \"‚ö†Ô∏è  MODERATE\"\n",
    "    desc = \"Decent baseline, further training recommended\"\n",
    "else:\n",
    "    rating = \"‚ùå NEEDS WORK\"\n",
    "    desc = \"Early training stage, more epochs needed\"\n",
    "\n",
    "report += f\"\"\"\n",
    "Overall Rating:          {rating}\n",
    "Assessment:              {desc}\n",
    "\n",
    "6. RECOMMENDATIONS\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "To improve accuracy beyond {EVAL_RESULTS['test_accuracy']*100:.1f}%:\n",
    "  ‚úì Train for more epochs ({CONFIG['epochs']} ‚Üí 50-100)\n",
    "  ‚úì Use lower initial learning rate (0.001)\n",
    "  ‚úì Implement curriculum learning (start with short sequences)\n",
    "  ‚úì Add data augmentation techniques\n",
    "  ‚úì Experiment with learning rate schedules\n",
    "\n",
    "To improve inference speed:\n",
    "  ‚úì Use batch inference (current optimal: batch={optimal_batch})\n",
    "  ‚úì Quantize model to INT8 ({ARCHITECTURE_INFO['model_size_mb_int8']:.1f} MB vs {ARCHITECTURE_INFO['model_size_mb_fp32']:.1f} MB)\n",
    "  ‚úì Consider TFLite conversion for mobile deployment\n",
    "  ‚úì Use GPU acceleration if available\n",
    "\n",
    "7. MODEL STATE\n",
    "{'‚îÄ'*70}\n",
    "\n",
    "Training Source:         {'From Scratch' if TRAINED_FROM_SCRATCH else 'Pre-trained Model'}\n",
    "Data Used:               Shakespeare.txt ({len(text):,} characters)\n",
    "Training Sequences:      {len(X_train):,}\n",
    "Validation Sequences:    {len(X_val):,}\n",
    "Test Sequences:          {len(X_test):,}\n",
    "\n",
    "{'‚ñà'*70}\n",
    "Report Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'‚ñà'*70}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report to file\n",
    "with open('evaluation_report_detailed.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n‚úì Detailed report saved to evaluation_report_detailed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Interactive Testing (Optional)\n",
    "\n",
    "Use the cells below to generate text with custom prompts and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interactive text generation with custom parameters\"\"\"\n",
    "\n",
    "# Set custom parameters here\n",
    "CUSTOM_PROMPT = \"To be or not\"\n",
    "CUSTOM_LENGTH = 300\n",
    "CUSTOM_TEMPERATURE = 0.8\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CUSTOM TEXT GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Parameters:\")\n",
    "print(f\"  Prompt: '{CUSTOM_PROMPT}'\")\n",
    "print(f\"  Length: {CUSTOM_LENGTH} characters\")\n",
    "print(f\"  Temperature: {CUSTOM_TEMPERATURE}\")\n",
    "\n",
    "print(f\"\\nüé≠ Generating text...\")\n",
    "custom_text = generate_text(\n",
    "    model=TRAINED_MODEL,\n",
    "    seed_text=CUSTOM_PROMPT,\n",
    "    length=CUSTOM_LENGTH,\n",
    "    temperature=CUSTOM_TEMPERATURE,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    seq_length=CONFIG['sequence_length']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Generated text:\")\n",
    "print(\"‚îÄ\"*70)\n",
    "print(custom_text)\n",
    "print(\"‚îÄ\"*70)\n",
    "\n",
    "# Analyze this generation\n",
    "custom_analysis = analyze_text(custom_text)\n",
    "print(f\"\\nüìä Analysis:\")\n",
    "print(f\"  Word count: {custom_analysis['word_count']}\")\n",
    "print(f\"  Unique words: {custom_analysis['unique_words']}\")\n",
    "print(f\"  Vocabulary diversity: {custom_analysis['vocabulary_diversity']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate multiple text samples for qualitative review\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BATCH GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate 5 samples with temperature 0.8\n",
    "prompt = \"The love of\"\n",
    "num_samples = 5\n",
    "\n",
    "print(f\"\\nüîÑ Generating {num_samples} samples with prompt: '{prompt}'\")\n",
    "print(f\"   Temperature: 0.8, Length: 200 characters\")\n",
    "print(f\"{'‚îÄ'*70}\\n\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = generate_text(\n",
    "        model=TRAINED_MODEL,\n",
    "        seed_text=prompt,\n",
    "        length=200,\n",
    "        temperature=0.8,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        seq_length=CONFIG['sequence_length']\n",
    "    )\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(sample[:180] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save all results and artifacts\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create results dictionary\n",
    "all_results = {\n",
    "    'evaluation': EVAL_RESULTS,\n",
    "    'benchmark': {k: dict(v) for k, v in BENCHMARK_RESULTS.items()},\n",
    "    'architecture': ARCHITECTURE_INFO,\n",
    "    'text_analysis': {\n",
    "        'generated': {k: v for k, v in TEXT_ANALYSIS['generated'].items() \n",
    "                     if k != 'char_distribution'},\n",
    "        'original': {k: v for k, v in TEXT_ANALYSIS['original'].items() \n",
    "                    if k != 'char_distribution'}\n",
    "    },\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open('test_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"‚úì Results saved to test_results.json\")\n",
    "\n",
    "# Save character mappings\n",
    "save_char_mappings(char_to_idx, idx_to_char)\n",
    "\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(\"  ‚îú‚îÄ benchmark_results.png\")\n",
    "print(\"  ‚îú‚îÄ text_quality_analysis.png\")\n",
    "print(\"  ‚îú‚îÄ model_parameters.png\")\n",
    "print(\"  ‚îú‚îÄ evaluation_report_detailed.txt\")\n",
    "print(\"  ‚îú‚îÄ test_results.json\")\n",
    "print(\"  ‚îî‚îÄ models/char_mappings.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ All results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
